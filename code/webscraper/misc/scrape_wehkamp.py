import cfscrape
import json
import time
import colorama
from datetime import datetime
from bs4 import BeautifulSoup

https://github.com/NotScar/zalando-scraper/blob/master/main.py

colorama.init()

REQUESTS_MANAGER = cfscrape.CloudflareScraper()
GET = REQUESTS_MANAGER.get
POST = REQUESTS_MANAGER.post
JSON_TO_TABLE = json.loads
TABLE_TO_JSON = json.dumps
COLOR = colorama.Fore

DISCORD_BASIC_LOGGING = False


WEBHOOKS = [
    # You can add as many webhooks as u want, diving them with ","
    "https://discord.com/api/webhooks/895422954404466790/UZZmbfvqnXjZCTqPc4Dt1vHuWifW4BD4IA6sPvPXXCiUj_SM0R6L56I31XB2r6R9POpb",
]

COUNTRY_LINKS = {
    "IT": "https://www.zalando.it/release-calendar/sneakers-uomo/",
    "UK": "https://www.zalando.co.uk/release-calendar/mens-shoes-sneakers/",
}

COUNTRY_BASE_URL = {"IT": "https://www.zalando.it/", "UK": "https://www.zalando.co.uk/"}

COLOR = colorama.Fore
LOGGING_COLORS = {
    "INFO": COLOR.CYAN,
    "LOG": COLOR.BLUE,
    "WARNING": COLOR.YELLOW,
    "ERROR": COLOR.RED,
}



def log(logType, message, details):
    logDate = str(datetime.now())

    logFile = open("logs.log", "a+")

    if len(details) == 0:
        logFile.write(logDate + " [%s] " % (logType) + message + "\n")
        print(
            logDate
            + LOGGING_COLORS[logType]
            + " [%s] " % (logType)
            + message
            + COLOR.RESET
        )
    else:
        logFile.write(
            logDate
            + " [%s] " % (logType)
            + message
            + " | "
            + TABLE_TO_JSON(details)
            + "\n"
        )
        print(
            logDate
            + LOGGING_COLORS[logType]
            + " [%s] " % (logType)
            + message
            + " | "
            + TABLE_TO_JSON(details)
            + COLOR.RESET
        )

    logFile.close()

    detailsString = ""

    if (logType == "LOG") and (DISCORD_BASIC_LOGGING == False):
        return

    for x in details:
        detailsString += "`%s = %s`\n" % (str(x), details[x])

    data = {
        "content": None,
        "embeds": [
            {
                "color": None,
                "fields": [
                    {"name": "LOG TYPE", "value": "`%s`" % (logType)},
                    {"name": "LOG MESSAGE", "value": "`%s`" % (message)},
                    {"name": "LOG DETAILS", "value": detailsString},
                    {"name": "TIME", "value": "`%s`" % (logDate)},
                ],
            }
        ],
        "username": "Zalando Scraper Logs",
        "avatar_url": "https://avatars.githubusercontent.com/u/1564818?s=280&v=4",
    }

    if len(details) == 0:
        data["embeds"][0]["fields"].remove(data["embeds"][0]["fields"][2])

    POST(LOGGING_WEBHOOK, json=data)


def save_external_articles(content):
    file = open("articles.json", "w+")
    file.write(TABLE_TO_JSON(content))
    file.close()
    return content


def load_external_articles():
    open("articles.json", "a+")
    file = open("articles.json", "r")
    fileContent = file.read()
    if len(fileContent) < 2:
        save_external_articles([])
        return []
    try:
        file.close()
        return JSON_TO_TABLE(fileContent)
    except:
        save_external_articles([])
        return []


def validate_country(countryCode):
    return not (COUNTRY_LINKS[countryCode] == None)


def get_page_data(countryCode):

    if validate_country(countryCode):
        response = GET(COUNTRY_LINKS[countryCode])
        if response.status_code == 200:
            return response.content
        else:
            log(
                "ERROR",
                "Error while retrieving page",
                {"statusCode": response.status_code},
            )
            return {"error": "Invalid Status Code", "status_code": response.status_code}
    log("ERROR", "Invalid Country (get_page_data)", {"countryCode": countryCode})
    return {"error": "Invalid Country"}


def filter_json(content):
    bs = BeautifulSoup(content, "html.parser")
    foundScripts = bs.find_all("script")

    for script in foundScripts:
        if len(script.contents) == 1:
            if script.contents[0].startswith("window.feedPreloadedState="):
                script = script.contents[0]
                script = script[26:]
                script = script[:-1]
                return JSON_TO_TABLE(script)["feed"]["items"]


def filter_articles(content):
    for articlesList in content:
        if articlesList["id"] == "products":
            return articlesList["articles"]


def filter_coming_soon(content):
    comingSoonList = []
    for article in content:
        if article["availability"]["comingSoon"] == True:
            comingSoonList.append(article)
    return comingSoonList


def adjust_articles_info(content, countryCode):
    adjustedArticlesList = []
    for article in content:
        articleInfo = {}
        rSplit = article["availability"]["releaseDate"].split(" ")
        rDate = rSplit[0].split("-")
        rTime = rSplit[1]
        articleInfo["zalandoId"] = article["id"]
        articleInfo["releaseDate"] = "%s-%s-%s %s" % (
            rDate[2],
            rDate[1],
            rDate[0],
            rTime,
        )
        articleInfo["productName"] = article["brand"] + " " + article["name"]
        articleInfo["originalPrice"] = article["price"]["original"]
        articleInfo["currentPrice"] = article["price"]["current"]
        articleInfo["link"] = "%s%s.html" % (
            COUNTRY_BASE_URL[countryCode],
            article["urlKey"],
        )
        articleInfo["imageUrl"] = article["imageUrl"]

        adjustedArticlesList.append(articleInfo)

    return adjustedArticlesList


def compare_articles(articles):
    if len(oldArticles) == 0:
        return articles
    else:
        if len(articles) == len(oldArticles):
            return []
        else:
            articlesToReturn = []
            for article in articles:
                found = False

                for article_ in oldArticles:

                    if article["zalandoId"] == article_["zalandoId"]:
                        found = True

                if found == False:
                    articlesToReturn.append(article)

            return articlesToReturn


def get_product_stock(link):
    response = GET(link)
    bs = BeautifulSoup(response.content, "html.parser")
    sizeArray = []
    try:
        sizeArray = JSON_TO_TABLE(
            bs.find("script", {"id": "z-vegas-pdp-props"}).contents[0][9:-3]
        )["model"]["articleInfo"]["units"]
    except:
        log("ERROR", "Could not retrieve model units and sizes", {"URL": link})

    sizeStockArray = []
    for x in sizeArray:
        sizeStockArray.append(
            {
                "size": x["size"]["local"],
                "sizeCountry": x["size"]["local_type"],
                "stock": x["stock"],
            }
        )

    return sizeStockArray


def send_message(content):

    for article in content:

        stocks = get_product_stock(article["link"])

        sizeString = ""
        countryString = ""
        stockString = ""
        totalStock = 0

        for size in stocks:
            sizeString += size["size"] + "\n"
            countryString += size["sizeCountry"] + "\n"
            stockString += str(size["stock"]) + "\n"
            totalStock += size["stock"]

        data = {
            "content": None,
            "embeds": [
                {
                    "description": "[%s](%s)"
                    % (article["productName"], article["link"]),
                    "color": None,
                    "fields": [
                        {
                            "name": "Price",
                            "value": article["currentPrice"],
                            "inline": True,
                        },
                        {
                            "name": "Release Date",
                            "value": article["releaseDate"],
                            "inline": True,
                        },
                        {"name": "Total Stock", "value": totalStock, "inline": True},
                    ],
                    "author": {"name": "Sneaker Drop", "url": article["link"]},
                    "thumbnail": {"url": article["imageUrl"]},
                },
                {
                    "color": None,
                    "fields": [
                        {"name": "Sizes", "value": sizeString, "inline": True},
                        {"name": "Country", "value": countryString, "inline": True},
                        {"name": "Stock", "value": stockString, "inline": True},
                    ],
                },
            ],
            "username": "á²¼",
            "avatar_url": "https://avatars.githubusercontent.com/u/1564818?s=280&v=4",
        }

        if len(stocks) == 0:
            data["embeds"].remove(data["embeds"][1])
            data["embeds"][0]["fields"][2]["value"] = "UNKNOWN"

        for webhook in WEBHOOKS:

            log(
                "LOG",
                "Article Message Sent",
                {"WEBHOOK": webhook, "ARTICLE-ID": article["zalandoId"]},
            )
            POST(webhook, json=data)


oldArticles = load_external_articles()


def main():
    global oldArticles
    country = "IT"
    articles = adjust_articles_info(
        filter_coming_soon(filter_articles(filter_json(get_page_data(country)))),
        country,
    )
    print(articles)
    exit(0)
    newArticles = compare_articles(articles)
    send_message(newArticles)
    save_external_articles(articles)
    oldArticles = articles


if __name__ == "__main__":
    log("INFO", "Zalando Scraper has been started", {})
    while True:
        main()
        time.sleep(2)
